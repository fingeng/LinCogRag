#!/usr/bin/env python3
"""
å¿«é€Ÿä¼˜åŒ–è„šæœ¬ - æ— éœ€ä¿®æ”¹æ ¸å¿ƒä»£ç çš„å‚æ•°è°ƒä¼˜
ç”¨äºå¿«é€Ÿæµ‹è¯•ä¸åŒè¶…å‚æ•°é…ç½®çš„æ•ˆæœ
"""

import json
import time
from datetime import datetime

# ä¼˜åŒ–é…ç½®æ–¹æ¡ˆ
OPTIMIZATION_CONFIGS = {
    "baseline": {
        "name": "åŸºçº¿é…ç½® (å½“å‰)",
        "max_iterations": 3,
        "iteration_threshold": 0.1,
        "top_k_sentence": 3,
        "retrieval_top_k": 32,
        "expected_speed": "150s/é—®é¢˜",
        "description": "å½“å‰ä½¿ç”¨çš„é…ç½®ï¼Œé€Ÿåº¦æœ€æ…¢ä½†å¯èƒ½æ•ˆæœæœ€å¥½"
    },
    
    "quick_fix": {
        "name": "å¿«é€Ÿä¼˜åŒ– (æ¨è)",
        "max_iterations": 2,
        "iteration_threshold": 0.3,
        "top_k_sentence": 5,
        "retrieval_top_k": 32,
        "expected_speed": "30-50s/é—®é¢˜",
        "description": "æé«˜é˜ˆå€¼+å‡å°‘è¿­ä»£ï¼Œé¢„è®¡æé€Ÿ3-5å€"
    },
    
    "aggressive": {
        "name": "æ¿€è¿›ä¼˜åŒ–",
        "max_iterations": 1,
        "iteration_threshold": 0.5,
        "top_k_sentence": 10,
        "retrieval_top_k": 32,
        "expected_speed": "10-20s/é—®é¢˜",
        "description": "åªåš1æ¬¡è¿­ä»£ï¼Œé€Ÿåº¦æœ€å¿«ä½†å¯èƒ½æŸå¤±æ•ˆæœ"
    },
    
    "balanced": {
        "name": "å¹³è¡¡é…ç½®",
        "max_iterations": 2,
        "iteration_threshold": 0.25,
        "top_k_sentence": 8,
        "retrieval_top_k": 32,
        "expected_speed": "20-40s/é—®é¢˜",
        "description": "é€Ÿåº¦å’Œæ•ˆæœçš„å¹³è¡¡ç‚¹"
    }
}


def print_optimization_guide():
    """æ‰“å°ä¼˜åŒ–æŒ‡å—"""
    print("=" * 80)
    print("LinearRAG åŒ»ç–—é¢†åŸŸä¼˜åŒ–æŒ‡å—")
    print("=" * 80)
    print("\nğŸ“Š å½“å‰æ€§èƒ½åˆ†æ:")
    print("   - æ£€ç´¢é€Ÿåº¦: 60-150ç§’/é—®é¢˜ (ä¸¥é‡è¿‡æ…¢)")
    print("   - é¢„è®¡æ€»æ—¶é—´: 21-53å°æ—¶ (1273ä¸ªé—®é¢˜)")
    print("   - ç“¶é¢ˆ: å›¾è§„æ¨¡è¿‡å¤§ (21ä¸‡å®ä½“) + è¿­ä»£è®¡ç®—æ˜‚è´µ")
    
    print("\n" + "=" * 80)
    print("ä¼˜åŒ–é…ç½®æ–¹æ¡ˆå¯¹æ¯”")
    print("=" * 80)
    
    for config_name, config in OPTIMIZATION_CONFIGS.items():
        print(f"\nã€{config['name']}ã€‘")
        print(f"   é…ç½®å: {config_name}")
        print(f"   max_iterations: {config['max_iterations']}")
        print(f"   iteration_threshold: {config['iteration_threshold']}")
        print(f"   top_k_sentence: {config['top_k_sentence']}")
        print(f"   é¢„æœŸé€Ÿåº¦: {config['expected_speed']}")
        print(f"   è¯´æ˜: {config['description']}")
    
    print("\n" + "=" * 80)
    print("å¿«é€Ÿæ“ä½œæŒ‡å—")
    print("=" * 80)
    print("\n1ï¸âƒ£ ä¿®æ”¹é…ç½®æ–‡ä»¶ (src/config.py):")
    print("   æ‰¾åˆ° LinearRAGConfig ç±»ï¼Œä¿®æ”¹é»˜è®¤å‚æ•°")
    print("\n2ï¸âƒ£ åœæ­¢å½“å‰è¿è¡Œ:")
    print("   kill 3478849  # æˆ–è€… pkill -f 'run.py'")
    print("\n3ï¸âƒ£ ä½¿ç”¨ä¼˜åŒ–é…ç½®é‡æ–°è¿è¡Œ:")
    print("   python run.py \\")
    print("       --use_hf_ner \\")
    print("       --embedding_model model/all-mpnet-base-v2 \\")
    print("       --dataset_name pubmed \\")
    print("       --llm_model gpt-4o-mini \\")
    print("       --max_workers 8 \\")
    print("       --use_mirage \\")
    print("       --mirage_dataset medqa \\")
    print("       --chunks_limit 10000 \\")
    print("       > medqa_optimized.log 2>&1 &")
    print("\n4ï¸âƒ£ ç›‘æ§æ€§èƒ½:")
    print("   tail -f medqa_optimized.log | grep 'Retrieving:'")
    
    print("\n" + "=" * 80)
    print("âš ï¸ é‡è¦æç¤º")
    print("=" * 80)
    print("1. å»ºè®®å…ˆç”¨ 'quick_fix' é…ç½®æµ‹è¯•100ä¸ªé—®é¢˜")
    print("2. å¯¹æ¯”é€Ÿåº¦å’Œå‡†ç¡®ç‡åï¼Œå†å†³å®šæ˜¯å¦è°ƒæ•´")
    print("3. å¦‚æœå‡†ç¡®ç‡ä¸‹é™<2%ï¼Œé€Ÿåº¦æå‡>3xï¼Œå°±å€¼å¾—é‡‡ç”¨")
    print("4. å¯ä»¥ç”¨å°æ•°æ®é›† (--questions_limit 100) å¿«é€ŸéªŒè¯")


def generate_config_file(config_name="quick_fix"):
    """ç”Ÿæˆä¼˜åŒ–åçš„é…ç½®æ–‡ä»¶"""
    config = OPTIMIZATION_CONFIGS[config_name]
    
    config_content = f'''from dataclasses import dataclass
"""
æ³¨æ„ï¼šè¯¥æ–‡ä»¶ä»…ç”¨äºå±•ç¤º/è®°å½•ä¸€æ¬¡å®éªŒé…ç½®ï¼Œä¸ä¾èµ–å…·ä½“ LLM wrapperã€‚
ä¸»æµç¨‹è¯·ä½¿ç”¨ `run.py` + `src/pipeline.py`ã€‚
"""

@dataclass
class LinearRAGConfig:
    """
    LinearRAGé…ç½® - {config['name']}
    ç”Ÿæˆæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
    é¢„æœŸé€Ÿåº¦: {config['expected_speed']}
    """
    def __init__(
        self,
        embedding_model,
        dataset_name,
        spacy_model="en_ner_bc5cdr_md",
        max_workers=8,
        llm_model=None,
        use_hf_ner=True,
        use_enhanced_ner=True,
        working_dir="import",
        batch_size=32,
        retrieval_top_k={config['retrieval_top_k']},  # âœ… ä¼˜åŒ–
        max_iterations={config['max_iterations']},  # âœ… ä¼˜åŒ–: å‡å°‘è¿­ä»£æ¬¡æ•°
        iteration_threshold={config['iteration_threshold']},  # âœ… ä¼˜åŒ–: æé«˜é˜ˆå€¼
        top_k_sentence={config['top_k_sentence']},  # âœ… ä¼˜åŒ–: å¢åŠ å¥å­æ•°
        passage_ratio=0.7,
        passage_node_weight=1.0,
        damping=0.85,
    ):
        # Model parameters
        self.embedding_model = embedding_model
        self.spacy_model = spacy_model
        self.llm_model = llm_model
        
        # NER strategy
        self.use_hf_ner = use_hf_ner
        self.use_enhanced_ner = use_enhanced_ner
        
        # Dataset parameters
        self.dataset_name = dataset_name
        self.working_dir = working_dir
        
        # Processing parameters
        self.max_workers = max_workers
        self.batch_size = batch_size
        
        # Retrieval parameters
        self.retrieval_top_k = retrieval_top_k
        self.max_iterations = max_iterations
        self.iteration_threshold = iteration_threshold
        self.top_k_sentence = top_k_sentence
        
        # Graph parameters
        self.passage_ratio = passage_ratio
        self.passage_node_weight = passage_node_weight
        self.damping = damping
'''
    
    output_path = f"src/config_optimized_{config_name}.py"
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(config_content)
    
    print(f"\nâœ… å·²ç”Ÿæˆä¼˜åŒ–é…ç½®æ–‡ä»¶: {output_path}")
    print(f"   é…ç½®: {config['name']}")
    print(f"   é¢„æœŸé€Ÿåº¦: {config['expected_speed']}")
    
    # ç”Ÿæˆä½¿ç”¨è¯´æ˜
    usage_content = f"""
# ä½¿ç”¨ {config['name']} é…ç½®

## 1. å¤‡ä»½åŸé…ç½®
cp src/config.py src/config_backup.py

## 2. æ›¿æ¢ä¸ºä¼˜åŒ–é…ç½®
cp {output_path} src/config.py

## 3. é‡æ–°è¿è¡Œ
kill $(pgrep -f "run.py")
python run.py \\
    --use_hf_ner \\
    --embedding_model model/all-mpnet-base-v2 \\
    --dataset_name pubmed \\
    --llm_model gpt-4o-mini \\
    --max_workers 8 \\
    --use_mirage \\
    --mirage_dataset medqa \\
    --chunks_limit 10000 \\
    --questions_limit 100 \\
    > medqa_{config_name}.log 2>&1 &

## 4. ç›‘æ§æ€§èƒ½
tail -f medqa_{config_name}.log

## 5. å¯¹æ¯”ç»“æœ
# å¯¹æ¯”é€Ÿåº¦: grep "Retrieving:" medqa_*.log
# å¯¹æ¯”å‡†ç¡®ç‡: ç­‰å¾…è¿è¡Œå®ŒæˆåæŸ¥çœ‹æœ€ç»ˆå‡†ç¡®ç‡
"""
    
    usage_path = f"usage_{config_name}.sh"
    with open(usage_path, 'w', encoding='utf-8') as f:
        f.write(usage_content)
    print(f"âœ… å·²ç”Ÿæˆä½¿ç”¨è¯´æ˜: {usage_path}")


def analyze_current_log():
    """åˆ†æå½“å‰æ—¥å¿—æ–‡ä»¶"""
    import re
    
    log_file = "medqa_full_fixed.log"
    
    try:
        with open(log_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æå–æ£€ç´¢è¿›åº¦
        retrieval_pattern = r'Retrieving:\s+(\d+)%.*?(\d+)/(\d+).*?\[([^<]+)<'
        matches = re.findall(retrieval_pattern, content)
        
        if matches:
            last_match = matches[-1]
            percent, current, total, elapsed = last_match
            print("\nğŸ“ˆ å½“å‰è¿›åº¦åˆ†æ:")
            print(f"   å·²å®Œæˆ: {current}/{total} ({percent}%)")
            print(f"   å·²ç”¨æ—¶: {elapsed}")
            
            # è®¡ç®—å¹³å‡é€Ÿåº¦
            if ',' in elapsed:
                parts = elapsed.split(',')
                hours = int(parts[0].split(':')[0])
                minutes = int(parts[0].split(':')[1])
                seconds = int(parts[0].split(':')[2])
                total_seconds = hours * 3600 + minutes * 60 + seconds
            else:
                time_parts = elapsed.split(':')
                if len(time_parts) == 3:
                    total_seconds = int(time_parts[0]) * 3600 + int(time_parts[1]) * 60 + int(time_parts[2])
                else:
                    total_seconds = int(time_parts[0]) * 60 + int(time_parts[1])
            
            avg_time = total_seconds / int(current)
            remaining = (int(total) - int(current)) * avg_time
            
            print(f"   å¹³å‡é€Ÿåº¦: {avg_time:.1f}ç§’/é—®é¢˜")
            print(f"   é¢„è®¡å‰©ä½™: {remaining/3600:.1f}å°æ—¶")
            
            # æä¾›å»ºè®®
            if avg_time > 100:
                print("\nâš ï¸ é€Ÿåº¦ä¸¥é‡è¿‡æ…¢ï¼Œå¼ºçƒˆå»ºè®®:")
                print("   1. ç«‹å³åœæ­¢å½“å‰è¿è¡Œ (kill 3478849)")
                print("   2. ä½¿ç”¨ 'quick_fix' é…ç½®é‡æ–°è¿è¡Œ")
                print("   3. é¢„è®¡å¯æé€Ÿè‡³ 30-50ç§’/é—®é¢˜")
            elif avg_time > 50:
                print("\nâš ï¸ é€Ÿåº¦è¾ƒæ…¢ï¼Œå»ºè®®ä¼˜åŒ–é…ç½®")
            else:
                print("\nâœ… é€Ÿåº¦å°šå¯")
        
        # æå–å›¾ç»Ÿè®¡ä¿¡æ¯
        if "Entity embeddings:" in content:
            entity_match = re.search(r'Entity embeddings: \((\d+),', content)
            passage_match = re.search(r'Passage embeddings: \((\d+),', content)
            sentence_match = re.search(r'Sentence embeddings: \((\d+),', content)
            
            if entity_match:
                print("\nğŸ“Š å›¾è§„æ¨¡ç»Ÿè®¡:")
                print(f"   å®ä½“æ•°: {entity_match.group(1)}")
                print(f"   æ–‡æ¡£æ•°: {passage_match.group(1)}")
                print(f"   å¥å­æ•°: {sentence_match.group(1)}")
                
                entities = int(entity_match.group(1))
                if entities > 200000:
                    print(f"\nâš ï¸ å®ä½“æ•°è¿‡å¤š ({entities:,})ï¼Œå»ºè®®:")
                    print("   1. è¿‡æ»¤ä½é¢‘å®ä½“ (å‡ºç°æ¬¡æ•°<3)")
                    print("   2. æé«˜ iteration_threshold é™åˆ¶æ‰©æ•£èŒƒå›´")
    
    except FileNotFoundError:
        print(f"âš ï¸ æœªæ‰¾åˆ°æ—¥å¿—æ–‡ä»¶: {log_file}")
    except Exception as e:
        print(f"âš ï¸ åˆ†ææ—¥å¿—æ—¶å‡ºé”™: {e}")


if __name__ == "__main__":
    print_optimization_guide()
    print("\n" + "=" * 80)
    
    # åˆ†æå½“å‰æ—¥å¿—
    analyze_current_log()
    
    print("\n" + "=" * 80)
    print("æ˜¯å¦ç”Ÿæˆä¼˜åŒ–é…ç½®æ–‡ä»¶?")
    print("=" * 80)
    print("\né€‰æ‹©é…ç½®æ–¹æ¡ˆ:")
    for i, (key, config) in enumerate(OPTIMIZATION_CONFIGS.items(), 1):
        print(f"   {i}. {config['name']} ({config['expected_speed']})")
    
    print("\næ¨è: 2 (å¿«é€Ÿä¼˜åŒ–)")
    print("è¾“å…¥æ•°å­—ç”Ÿæˆé…ç½®ï¼Œæˆ–æŒ‰ Enter è·³è¿‡:")
    
    choice = input().strip()
    
    if choice:
        config_list = list(OPTIMIZATION_CONFIGS.keys())
        try:
            idx = int(choice) - 1
            if 0 <= idx < len(config_list):
                config_name = config_list[idx]
                generate_config_file(config_name)
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©")
        except ValueError:
            print("âŒ è¯·è¾“å…¥æ•°å­—")
    
    print("\nâœ… å®Œæˆ! è¯·æŸ¥çœ‹ç”Ÿæˆçš„æ–‡ä»¶å’Œè¯´æ˜")
