#!/usr/bin/env python3
"""
å¢é‡æ–¹å¼ï¼šå°†PubMedQAçš„500ä¸ªCONTEXTS chunksæ·»åŠ åˆ°å·²æœ‰çš„å›¾ä¸­
è¿™æ ·å¯ä»¥å¿«é€ŸéªŒè¯ï¼Œè€Œä¸éœ€è¦é‡å»ºæ•´ä¸ª50kçš„å›¾
"""

import json
import os
import pickle
import argparse
from pathlib import Path

def load_existing_graph_info(graph_dir='import/pubmed_mirage_medqa'):
    """æ£€æŸ¥ç°æœ‰å›¾çš„ä¿¡æ¯"""
    
    print("="*80)
    print("Checking Existing Graph")
    print("="*80)
    
    if not os.path.exists(graph_dir):
        print(f"âŒ Graph directory not found: {graph_dir}")
        return None
    
    # æ£€æŸ¥æ–‡ä»¶
    files = {
        'ner_results': 'ner_results.json',
        'passage_embedding': 'passage_embedding.parquet',
        'sentence_embedding': 'sentence_embedding.parquet',
        'entity_embedding': 'entity_embedding.parquet',
        'graphml': 'LinearRAG.graphml',
    }
    
    info = {'dir': graph_dir, 'files': {}}
    
    for name, filename in files.items():
        filepath = os.path.join(graph_dir, filename)
        if os.path.exists(filepath):
            size = os.path.getsize(filepath)
            info['files'][name] = {
                'path': filepath,
                'size': size,
                'size_mb': size / (1024*1024)
            }
            print(f"âœ… {filename}: {size/(1024*1024):.1f} MB")
        else:
            print(f"âŒ {filename}: Not found")
            info['files'][name] = None
    
    # è¯»å–NERç»“æœæ£€æŸ¥passageæ•°é‡
    if info['files'].get('ner_results'):
        with open(info['files']['ner_results']['path'], 'r') as f:
            ner_data = json.load(f)
        info['num_passages'] = len(ner_data)
        print(f"\nğŸ“Š Current passages in graph: {info['num_passages']}")
    
    return info

def load_pubmedqa_chunks(chunks_file='pubmedqa_contexts_chunks.jsonl'):
    """åŠ è½½PubMedQA chunks"""
    
    print("\n" + "="*80)
    print("Loading PubMedQA Chunks")
    print("="*80)
    
    if not os.path.exists(chunks_file):
        print(f"âŒ Chunks file not found: {chunks_file}")
        print(f"   Please run: python extract_pubmedqa_contexts.py")
        return None
    
    chunks = []
    with open(chunks_file, 'r') as f:
        for line in f:
            chunks.append(json.loads(line))
    
    print(f"âœ… Loaded {len(chunks)} PubMedQA chunks")
    print(f"   Average length: {sum(len(c['text']) for c in chunks)/len(chunks):.0f} chars")
    
    return chunks

def create_augmented_corpus(original_corpus_files, pubmedqa_chunks, output_file):
    """
    åˆ›å»ºå¢å¼ºçš„corpusï¼šåŸå§‹50k + PubMedQA 500
    ä½†ä¸å®é™…åˆå¹¶å¤§æ–‡ä»¶ï¼Œè€Œæ˜¯åˆ›å»ºä¸€ä¸ªæŒ‡å‘æ–‡ä»¶çš„åˆ—è¡¨
    """
    
    print("\n" + "="*80)
    print("Creating Augmented Corpus Configuration")
    print("="*80)
    
    config = {
        'original_corpus': original_corpus_files,
        'pubmedqa_chunks_file': 'pubmedqa_contexts_chunks.jsonl',
        'total_original': 50000,  # ä¼°è®¡
        'total_pubmedqa': len(pubmedqa_chunks),
        'total': 50000 + len(pubmedqa_chunks),
    }
    
    # ä¿å­˜é…ç½®
    config_file = 'corpus_augmented_config.json'
    with open(config_file, 'w') as f:
        json.dump(config, f, indent=2)
    
    print(f"âœ… Created augmented corpus configuration")
    print(f"   Original corpus: ~{config['total_original']} passages")
    print(f"   PubMedQA chunks: {config['total_pubmedqa']} passages")
    print(f"   Total: {config['total']} passages")
    print(f"   Config saved to: {config_file}")
    
    return config

def create_quick_test_script(output_file='test_pubmedqa_with_contexts.sh'):
    """
    åˆ›å»ºå¿«é€Ÿæµ‹è¯•è„šæœ¬
    å…ˆç”¨500ä¸ªPubMedQA chunksæ„å»ºå°å›¾æµ‹è¯•
    """
    
    script = """#!/bin/bash

# å¿«é€Ÿæµ‹è¯•ï¼šåªç”¨500ä¸ªPubMedQA CONTEXTSæ„å»ºå›¾

echo "============================================================"
echo "Quick Test: PubMedQA with CONTEXTS corpus"
echo "============================================================"

# Step 1: å‡†å¤‡corpusç›®å½•
echo "Step 1: Preparing corpus..."
mkdir -p dataset/pubmed_pubmedqa/chunk
cp pubmedqa_contexts_chunks.jsonl dataset/pubmed_pubmedqa/chunk/pubmed.jsonl

# Step 2: æ„å»ºå›¾ï¼ˆ500 passagesï¼‰
echo ""
echo "Step 2: Building graph (this will take 5-10 minutes)..."
python run.py \\
    --dataset_name pubmed_pubmedqa \\
    --dataset pubmedqa \\
    --mirage_dataset pubmedqa \\
    --llm_name gpt-3.5-turbo \\
    --retrieval_method linearrag \\
    --top_k 32 \\
    --build_graph

# Step 3: è¿è¡Œæµ‹è¯•ï¼ˆå‰50ä¸ªé—®é¢˜ï¼‰
echo ""
echo "Step 3: Running test on first 50 questions..."
python run.py \\
    --dataset_name pubmed_pubmedqa \\
    --dataset pubmedqa \\
    --mirage_dataset pubmedqa \\
    --llm_name gpt-3.5-turbo \\
    --retrieval_method linearrag \\
    --top_k 32 \\
    --max_samples 50

echo ""
echo "============================================================"
echo "Test complete! Check the results."
echo "Expected improvements:"
echo "  - Retrieval scores: 0.001 -> 0.1-0.3 (100x-300x better)"
echo "  - Accuracy: ~0% -> 60-80%"
echo "============================================================"
"""
    
    with open(output_file, 'w') as f:
        f.write(script)
    
    os.chmod(output_file, 0o755)
    
    print(f"\nâœ… Created quick test script: {output_file}")
    print(f"   Run with: ./{output_file}")

def main():
    parser = argparse.ArgumentParser(description='Add PubMedQA chunks to existing graph')
    parser.add_argument('--graph-dir', type=str, default='import/pubmed_mirage_medqa',
                        help='Existing graph directory')
    parser.add_argument('--chunks-file', type=str, default='pubmedqa_contexts_chunks.jsonl',
                        help='PubMedQA chunks file')
    
    args = parser.parse_args()
    
    print("\nğŸš€ PubMedQA Graph Augmentation Pipeline\n")
    
    # Step 1: æ£€æŸ¥ç°æœ‰å›¾
    graph_info = load_existing_graph_info(args.graph_dir)
    
    # Step 2: åŠ è½½PubMedQA chunks
    pubmedqa_chunks = load_pubmedqa_chunks(args.chunks_file)
    
    if not pubmedqa_chunks:
        print("\nâŒ Failed to load PubMedQA chunks")
        return
    
    # Step 3: åˆ›å»ºå¿«é€Ÿæµ‹è¯•è„šæœ¬
    create_quick_test_script()
    
    # æ€»ç»“
    print("\n" + "="*80)
    print("Summary & Recommendations")
    print("="*80)
    
    print("""
æˆ‘ä»¬æœ‰ä¸¤ä¸ªæ–¹æ¡ˆå¯ä»¥æµ‹è¯•ï¼š

æ–¹æ¡ˆA: å¿«é€ŸéªŒè¯ - åªç”¨500ä¸ªPubMedQA chunks (æ¨èå…ˆåš)
====================================================
ä¼˜ç‚¹ï¼š
  âœ… å¿«é€Ÿï¼ˆ5-10åˆ†é’Ÿæ„å»ºå›¾ï¼‰
  âœ… èƒ½éªŒè¯æ ¸å¿ƒå‡è®¾ï¼šLinearRAGèƒ½å¦ä»æ­£ç¡®çš„corpusæ£€ç´¢åˆ°å¯¹åº”æ–‡æ¡£
  âœ… 100% corpusè¦†ç›–ï¼ˆæ‰€æœ‰500ä¸ªé—®é¢˜çš„CONTEXTSéƒ½åœ¨corpusä¸­ï¼‰

æ­¥éª¤ï¼š
  1. è¿è¡Œ: ./test_pubmedqa_with_contexts.sh
  2. æŸ¥çœ‹æ£€ç´¢åˆ†æ•°æ˜¯å¦æå‡ï¼ˆ0.001 -> 0.1+ï¼‰
  3. æŸ¥çœ‹å‡†ç¡®ç‡æ˜¯å¦æå‡ï¼ˆ0% -> 60-80%ï¼‰

é¢„æœŸç»“æœï¼š
  å¦‚æœå‡è®¾æ­£ç¡®ï¼Œåº”è¯¥çœ‹åˆ°ï¼š
  - æ£€ç´¢åˆ†æ•°æ˜¾è‘—æå‡ï¼ˆ100-300å€ï¼‰
  - å‡†ç¡®ç‡ä»0%æå‡åˆ°60-80%
  - è¯æ˜corpusåŒ¹é…çš„é‡è¦æ€§


æ–¹æ¡ˆB: å®Œæ•´æµ‹è¯• - 50k + 500 chunks
===================================
ä¼˜ç‚¹ï¼š
  âœ… æ›´çœŸå®çš„åœºæ™¯ï¼ˆå¤§è§„æ¨¡corpusï¼‰
  âœ… æµ‹è¯•LinearRAGåœ¨å¤§corpusä¸­çš„æ£€ç´¢èƒ½åŠ›

ç¼ºç‚¹ï¼š
  âŒ éœ€è¦é‡å»º50kçš„å›¾ï¼ˆå‡ å°æ—¶ï¼‰
  âŒ è®¡ç®—æˆæœ¬é«˜

æ­¥éª¤ï¼š
  1. å°†pubmedqa_contexts_chunks.jsonlè¿½åŠ åˆ°åŸå§‹corpus
  2. åˆ é™¤æ—§å›¾: rm -rf import/pubmed_mirage_medqa
  3. é‡å»ºå›¾: python run.py --dataset_name pubmed --dataset medqa --build_graph
  4. æµ‹è¯•: python run.py --dataset pubmedqa --mirage_dataset pubmedqa


å»ºè®®é¡ºåºï¼š
=========
1. å…ˆè¿è¡Œæ–¹æ¡ˆAï¼ˆå¿«é€ŸéªŒè¯ï¼Œ10åˆ†é’Ÿï¼‰
2. å¦‚æœæ–¹æ¡ˆAæˆåŠŸï¼Œè¯´æ˜å‡è®¾æ­£ç¡®
3. å†è€ƒè™‘æ˜¯å¦è¿è¡Œæ–¹æ¡ˆBï¼ˆå®Œæ•´æµ‹è¯•ï¼Œå‡ å°æ—¶ï¼‰

æ–¹æ¡ˆAè¶³ä»¥å›ç­”æ ¸å¿ƒé—®é¢˜ï¼š
  "å¦‚æœcorpusåŒ…å«æ­£ç¡®æ–‡æ¡£ï¼ŒLinearRAGèƒ½å¦æ£€ç´¢åˆ°ï¼Ÿ"
""")
    
    print("\n" + "="*80)
    print("Ready to test! Run: ./test_pubmedqa_with_contexts.sh")
    print("="*80)

if __name__ == '__main__':
    main()
