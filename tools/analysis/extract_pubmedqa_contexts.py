#!/usr/bin/env python3
"""
æ–¹æ¡ˆ2: ç›´æ¥ä½¿ç”¨PubMedQAè‡ªå¸¦çš„CONTEXTSä½œä¸ºcorpus
è¿™æ˜¯æœ€å¿«é€Ÿçš„æ–¹æ³•ï¼Œå› ä¸ºCONTEXTSå·²ç»æ˜¯é«˜è´¨é‡çš„ç»“æ„åŒ–æ‘˜è¦
"""

import json
import os
from collections import defaultdict

def extract_contexts_as_chunks(data_file='MIRAGE/rawdata/pubmedqa/data/test_set.json'):
    """
    ä»PubMedQAæ•°æ®é›†ä¸­æå–CONTEXTSä½œä¸ºchunks
    æ¯ä¸ªPMIDçš„CONTEXTSç»„åˆæˆä¸€ä¸ªchunk
    """
    
    print("="*80)
    print("Extracting CONTEXTS from PubMedQA")
    print("="*80)
    
    with open(data_file, 'r') as f:
        data = json.load(f)
    
    chunks = []
    
    for pmid, item in data.items():
        question = item['QUESTION']
        contexts = item['CONTEXTS']
        labels = item.get('LABELS', [])
        answer = item['final_decision']
        long_answer = item.get('LONG_ANSWER', '')
        
        # æ–¹æ¡ˆA: å°†æ‰€æœ‰CONTEXTSåˆå¹¶ä¸ºä¸€ä¸ªchunk
        combined_text = ' '.join(contexts)
        
        chunk = {
            'pmid': pmid,
            'text': combined_text,
            'question': question,
            'answer': answer,
            'long_answer': long_answer,
            'contexts': contexts,  # ä¿ç•™åŸå§‹ç»“æ„
            'labels': labels,
            'source': 'pubmedqa_contexts',
        }
        
        chunks.append(chunk)
    
    print(f"\nâœ… Extracted {len(chunks)} chunks from PubMedQA CONTEXTS")
    
    # ç»Ÿè®¡
    total_chars = sum(len(c['text']) for c in chunks)
    avg_chars = total_chars / len(chunks)
    
    print(f"   Average chunk length: {avg_chars:.0f} characters")
    print(f"   Total text: {total_chars:,} characters")
    
    return chunks

def save_chunks_jsonl(chunks, output_file='pubmedqa_contexts_chunks.jsonl'):
    """ä¿å­˜ä¸ºJSONLæ ¼å¼"""
    
    with open(output_file, 'w') as f:
        for chunk in chunks:
            f.write(json.dumps(chunk) + '\n')
    
    print(f"âœ… Saved to: {output_file}")
    
    return output_file

def merge_with_original_corpus(pubmedqa_chunks_file, 
                                original_corpus_file='dataset/pubmed/chunk/pubmed.jsonl',
                                output_file='dataset/pubmed/chunk/pubmed_with_pubmedqa.jsonl'):
    """
    å°†PubMedQA chunksä¸åŸå§‹50k corpusåˆå¹¶
    """
    
    print("\n" + "="*80)
    print("Merging with Original Corpus")
    print("="*80)
    
    # è¯»å–åŸå§‹corpus
    if not os.path.exists(original_corpus_file):
        print(f"âš ï¸  Warning: Original corpus not found at {original_corpus_file}")
        print(f"   Will create new corpus with only PubMedQA chunks")
        original_chunks = []
    else:
        original_chunks = []
        with open(original_corpus_file, 'r') as f:
            for line in f:
                original_chunks.append(json.loads(line))
        print(f"âœ… Loaded original corpus: {len(original_chunks)} chunks")
    
    # è¯»å–PubMedQA chunks
    pubmedqa_chunks = []
    with open(pubmedqa_chunks_file, 'r') as f:
        for line in f:
            pubmedqa_chunks.append(json.loads(line))
    print(f"âœ… Loaded PubMedQA chunks: {len(pubmedqa_chunks)} chunks")
    
    # åˆå¹¶
    all_chunks = original_chunks + pubmedqa_chunks
    print(f"\nğŸ“Š Total chunks after merge: {len(all_chunks)}")
    print(f"   Original: {len(original_chunks)}")
    print(f"   PubMedQA: {len(pubmedqa_chunks)}")
    
    # ä¿å­˜
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    with open(output_file, 'w') as f:
        for chunk in all_chunks:
            f.write(json.dumps(chunk) + '\n')
    
    print(f"\nâœ… Saved merged corpus to: {output_file}")
    
    return output_file

def show_sample_chunks(chunks_file, n=3):
    """æ˜¾ç¤ºæ ·ä¾‹chunks"""
    
    print("\n" + "="*80)
    print("Sample Chunks")
    print("="*80)
    
    with open(chunks_file, 'r') as f:
        chunks = [json.loads(line) for line in f]
    
    for i, chunk in enumerate(chunks[:n]):
        print(f"\nChunk {i+1}:")
        print(f"  PMID: {chunk.get('pmid', 'N/A')}")
        print(f"  Question: {chunk.get('question', 'N/A')[:80]}...")
        print(f"  Answer: {chunk.get('answer', 'N/A')}")
        
        if 'labels' in chunk:
            print(f"  Structure: {' -> '.join(chunk['labels'])}")
        
        print(f"  Text length: {len(chunk['text'])} chars")
        print(f"  Text preview: {chunk['text'][:200]}...")
        print("-"*80)

def main():
    """
    ä¸»æµç¨‹ï¼š
    1. ä»PubMedQAæå–CONTEXTSä½œä¸ºchunks
    2. ä¿å­˜ä¸ºJSONLæ ¼å¼
    3. ä¸åŸå§‹50k corpusåˆå¹¶
    4. æ˜¾ç¤ºæ ·ä¾‹
    """
    
    print("\nğŸš€ PubMedQA CONTEXTS Extraction Pipeline\n")
    
    # Step 1: æå–CONTEXTS
    chunks = extract_contexts_as_chunks()
    
    # Step 2: ä¿å­˜chunks
    chunks_file = save_chunks_jsonl(chunks)
    
    # Step 3: æ˜¾ç¤ºæ ·ä¾‹
    show_sample_chunks(chunks_file)
    
    # Step 4: ä¸åŸå§‹corpusåˆå¹¶
    merged_file = merge_with_original_corpus(chunks_file)
    
    # æ€»ç»“
    print("\n" + "="*80)
    print("âœ… Pipeline Complete!")
    print("="*80)
    
    print(f"""
Files created:
  1. PubMedQA chunks: {chunks_file}
  2. Merged corpus: {merged_file}

Next steps to rebuild graph with augmented corpus:

æ–¹æ¡ˆA: ä½¿ç”¨ç°æœ‰çš„pubmed_mirage_medqaå›¾ï¼Œæ·»åŠ 500ä¸ªchunks
-------------------------------------------------------
1. å°†merged corpusæ”¾åˆ°æ­£ç¡®ä½ç½®:
   cp {merged_file} dataset/pubmed/chunk/pubmed.jsonl
   
2. åˆ é™¤æ—§å›¾ï¼ˆè§¦å‘é‡å»ºï¼‰:
   rm -rf import/pubmed_mirage_medqa
   
3. é‡æ–°è¿è¡ŒMedQAï¼ˆä¼šè‡ªåŠ¨é‡å»ºå›¾ï¼‰:
   python run.py --dataset_name pubmed \\
                 --dataset medqa \\
                 --mirage_dataset medqa \\
                 --build_graph

4. ç„¶åæµ‹è¯•PubMedQAï¼ˆå¤ç”¨è¿™ä¸ªå›¾ï¼‰:
   python run.py --dataset_name pubmed \\
                 --dataset pubmedqa \\
                 --mirage_dataset pubmedqa \\
                 --llm_name gpt-3.5-turbo \\
                 --retrieval_method linearrag \\
                 --top_k 32

æ–¹æ¡ˆB: å…ˆç”¨500ä¸ªPubMedQA chunksæµ‹è¯•ï¼ˆå¿«é€ŸéªŒè¯ï¼‰
------------------------------------------------
1. åˆ›å»ºå°å‹æµ‹è¯•corpus:
   head -n 500 {chunks_file} > dataset/pubmed/chunk/pubmed_test.jsonl
   
2. ä¿®æ”¹run.pyæŒ‡å‘æµ‹è¯•corpus

3. æ„å»ºå°å›¾æµ‹è¯•

è¿™æ ·å¯ä»¥å¿«é€ŸéªŒè¯ï¼š
  - å¦‚æœcorpusåŒ…å«æ­£ç¡®æ–‡æ¡£ï¼Œæ£€ç´¢åˆ†æ•°æ˜¯å¦æå‡ï¼Ÿ
  - LinearRAGèƒ½å¦ç­›é€‰å‡ºå¯¹åº”çš„CONTEXTSï¼Ÿ
  - å‡†ç¡®ç‡æ˜¯å¦æ˜¾è‘—æå‡ï¼Ÿ

é¢„æœŸç»“æœï¼š
  - æ£€ç´¢åˆ†æ•°: 0.001 â†’ 0.1-0.3 (æå‡100-300å€)
  - å‡†ç¡®ç‡: ~0% â†’ 60-80%
""")

if __name__ == '__main__':
    main()
