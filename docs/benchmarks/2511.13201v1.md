# Cog-RAG: Cognitive-Inspired Dual-Hypergraph with Theme Alignment Retrieval-Augmented Generation 

Hao Hu ${ }^{1}$, Yifan Feng ${ }^{2}$, Ruoxue Li ${ }^{3}$, Rundong Xue ${ }^{1}$, Xingliang Hou ${ }^{4}$, Zhiqiang Tian ${ }^{4}$, Yue $\mathrm{Gao}^{2}$, Shaoyi Du ${ }^{1 *}$,<br>${ }^{1}$ State Key Laboratory of Human-Machine Hybrid Augmented Intelligence, National Engineering Research Center for Visual Information and Applications, and Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University<br>${ }^{2}$ BNRist, THUIBCS, BLBCI, School of Software, Tsinghua University<br>${ }^{3}$ School of Artificial Intelligence, Xidian University<br>${ }^{4}$ School of Software Engineering, Xi'an Jiaotong University<br>huhao@stu.xjtu.edu.cn, dushaoyi@xjtu.edu.cn


#### Abstract

Retrieval-Augmented Generation (RAG) enhances the response quality and domain-specific performance of large language models (LLMs) by incorporating external knowledge to combat hallucinations. In recent research, graph structures have been integrated into RAG to enhance the capture of semantic relations between entities. However, it primarily focuses on low-order pairwise entity relations, limiting the high-order associations among multiple entities. Hypergraphenhanced approaches address this limitation by modeling multi-entity interactions via hyperedges, but they are typically constrained to inter-chunk entity-level representations, overlooking the global thematic organization and alignment across chunks. Drawing inspiration from the top-down cognitive process of human reasoning, we propose a theme-aligned dual-hypergraph RAG framework (Cog-RAG) that uses a theme hypergraph to capture inter-chunk thematic structure and an entity hypergraph to model high-order semantic relations. Furthermore, we design a cognitive-inspired two-stage retrieval strategy that first activates query-relevant thematic content from the theme hypergraph, and then guides finegrained recall and diffusion in the entity hypergraph, achieving semantic alignment and consistent generation from global themes to local details. Our extensive experiments demonstrate that Cog-RAG significantly outperforms existing state-of-the-art baseline approaches.


## Introduction

Retrieval-Augmented Generation (RAG) has recently gained increasing attention for enhancing the performance of large language models (LLMs) on knowledge-intensive tasks (Lewis et al. 2020; Gao et al. 2023; Li et al. 2024). It combats LLMs' hallucination by incorporating external knowledge, thereby enhancing response quality and reliability (Ayala and Bechard 2024; Xia et al. 2025). Moreover, it enables integration with private or domain-specific knowledge bases, thereby increasing the model's adaptability to vertical domains. With these advantages, RAG has emerged as a fundamental component in question answering, doc-

[^0]![](https://cdn.mathpix.com/cropped/7129fb25-f695-45a3-839a-459d6159495b-01.jpg?height=515&width=836&top_left_y=814&top_left_x=1103)
Figure 1: Knowledge modeling of graph, hypergraph, and our theme-enhanced RAG.

ument understanding, and intelligent assistants (Fan et al. 2024; Dong et al. 2025).

Despite the notable potential of RAG in enhancing LLMs' response quality, current methods mostly rely on a flattened chunk-based retrieval that matches queries to document chunks via vector similarity (Asai et al. 2023; Yang et al. 2024). However, this fails to capture inter-chunk dependencies and semantic hierarchies, resulting in fragmented and weakly connected retrieval content, which weakens the model's structured understanding of the entire knowledge.

To address this, recent studies have attempted to introduce graph structures into RAG framework, aiming to construct corpus-wide knowledge graphs that capture the structural semantic relations between entities (Peng et al. 2024; Zhang et al. 2025; Wang et al. 2025). For instance, GraphRAG (Edge et al. 2024) and LightRAG (Guo et al. 2024) utilize graph structures to strengthen entity-level indexing and retrieval, explicitly capturing semantic relations to improve information organization. Hyper-RAG (Feng et al. 2025a) uses hypergraphs to model complex relations between multiple entities. Nevertheless, these approaches primarily concentrate on entity-level structural modeling and lack a unified organization of knowledge themes and semantic-driven reasoning, making it difficult to support hierarchical integration
of information from macro comprehension to micro details.
It is worth noting that humans tend to follow a top-down information processing path when handling complex tasks (Cheng et al. 2025; Gutiérrez et al. 2024). They begin by identifying the core themes of the problem and constructing a global semantic scaffold. Based on this, they recall and integrate relevant details to form a coherent and structured response. This "theme-driven, detail-recall" cognitive pattern reflects the inherent hierarchical organization and semantic coherence in human information processing.

Inspired by this cognitive insight, we propose a dualhypergraph with theme alignment RAG framework (CogRAG). Figure 1 shows its difference with other methods in knowledge modeling. Our method leverages a dualhypergraph structure to model the global theme structure and fine-grained high-order semantic relations. In addition, it introduces a cognitive-inspired two-stage retrieval strategy that simulates the human top-down information comprehension process, thereby enhancing the semantic consistency and structural expressiveness of generated responses. The main contributions are summarized as follows:

- We propose Cog-RAG that simulates the human topdown information processing path, enabling hierarchical generation modeling from macro-level semantic comprehension to micro-level information integration.
- We design a dual-hypergraph semantic indexing scheme to separately model global inter-chunk theme structure and intra-chunk fine-grained high-order semantic relations, overcoming the limitations of prior graphenhanced RAG models that focus only on pairwise relations and lack unified thematic organization.
- We develop a cognitive-inspired two-stage retrieval strategy that first activates relevant context in the theme hypergraph and then triggers detail recall and diffusion in the entity hypergraph. This "theme-driven, detail-recall" process enables semantic alignment across granularity and significantly improves the coherence and quality of the response.


## Related Work

## RAG with Knowledge Graph

Most text-based RAG methods (Asai et al. 2023; Zhang et al. 2024; Xia et al. 2025; Yang et al. 2024) rely on a flattened paragraph structure, which makes it difficult to model semantic associations and contextual dependencies across text chunks, thereby limiting the accuracy and completeness of generated responses. To address this issue, recent studies (Sarmah et al. 2024; Peng et al. 2024) have explored knowledge graphs within the RAG framework to structurally represent entities and relations, aiming to enhance the organization and semantic expressiveness of retrieved content.

Some recent studies (Gutiérrez et al. 2024; Li and Du 2023; Cheng et al. 2025) attempt to automatically extract knowledge graph triples from the corpus and retrieve relevant subgraphs to improve content relevance and interpretability. However, these methods typically construct sparse graph structures, making it difficult to capture the full
semantic space and contextual dependencies. To address the semantic sparsity issue and better model the semantic structure of documents, graph-enhanced RAG approaches (Edge et al. 2024; Guo et al. 2024; Chen et al. 2025) extract entities and their relations, and directly build document-level graph databases enriched with contextual information, thereby reducing information loss during the text-to-graph conversion process. GraphRAG and LightRAG employ LLMs to extract entities and relations from texts as vertices and edges of the graph. Nevertheless, existing methods primarily focus on low-order pairwise relations between entities, neglecting high-order group associations and global topic modeling, which limits the semantic coverage and structural expressiveness of the generated content.

## Hypergraph

Hypergraphs connect multiple vertices via hyperedges, effectively modeling complex high-order relationships among entities and overcoming the limitation of conventional graphs, which support only binary relations (Gao et al. 2022; Feng et al. 2025b). These strong modeling capabilities have led to significant progress in fields such as recommender systems, social network analysis, and brain network modeling (Ji et al. 2020; Sun et al. 2023; Han et al. 2025). However, in the RAG framework, existing research is constrained to graph structures, primarily focusing on the pairwise relationships between entities. To model multiple entity group semantic associations, GraphRAG generates community reports through the semantic clustering of entities, while $\mathrm{Hi}-$ RAG (Huang et al. 2025) incorporates hierarchical graph knowledge via multi-level clustering. While effective in capturing local relationships, these methods rely on discrete category divisions and fail to model higher-order dependencies, resulting in information loss.

In contrast, hypergraphs naturally connect multiple entities through hyperedges, allowing them to interact with multiple hyperedges at once. This enables the capture of higherorder dependencies in a unified framework. It avoids the fragmentation and loss of information typical in clustering approaches and maximizes the retention of semantic information during text-to-graph conversion (Feng et al. 2025a; Luo et al. 2025). The hypergraph structure enhances semantic associations both within and across documents, thereby improving the RAG system's ability to understand context and ensure consistency in generated responses.

## Preliminary

In this section, we provide a general expression for RAG and graph-enhanced RAG, referring to the definitions in (Edge et al. 2024; Guo et al. 2024).

An RAG system $\mathcal{M}$ generally includes LLM, retriever, and corpora, which can be defined as follows:

$$
\mathcal{M}=(\operatorname{LLM}, \mathcal{R}(q, \mathcal{D}))
$$

Given a query $q$, the retriever $\mathcal{R}$ selects relevant contexts from the corpora $\mathcal{D}$, which are then used by the LLM to generate a response.

![](https://cdn.mathpix.com/cropped/7129fb25-f695-45a3-839a-459d6159495b-03.jpg?height=1092&width=1661&top_left_y=178&top_left_x=234)
Figure 2: The overall framework of Cog-RAG.

For the graph-enhanced RAG, the corpus is organized into a graph structure, where vertices represent entities and edges represent the relations. It can be formally defined as follows:

$$
\mathcal{M}=(L L M, \mathcal{R}(q, \mathcal{D}=\{\mathcal{V}, \mathcal{E}\})) .
$$

The query $q$ retrieves relevant vertices or edges from the graph-structured corpus $\mathcal{D}=\{\mathcal{V}, \mathcal{E}\}$, enabling the LLM to respond.

## Method

## Overview

As illustrated in Figure 2, Cog-RAG comprises two main components: dual-hypergraph indexing and cognitiveinspired two-stage retrieval. We construct the dualhypergraph with complementary semantic granularity: the theme hypergraph captures semantic theme associations between chunks (such as storyline, narrative outline, and summary), providing global semantic theme organization; the entity hypergraph models fine-grained high-order relations among entities (such as persons, concepts, and events), supporting local semantic relations. In the retrieval stage, mimicking the human "top-down" reasoning pattern, Cog-RAG first activates relevant themes in the theme hypergraph as global semantic anchors. Guided by these anchors, it then retrieves related entities and relations information from the entity hypergraph. The final response is generated via LLMs, utilizing theme-driven, detail-recall knowledge as evidence.

## Dual-Hypergraph Indexing

To more effectively model complex high-order associations among multiple entities in corpora and avoid the information loss by graph structure, we introduce hypergraphs for modeling. The general formulation is defined as follows:

$$
\mathcal{M}=\left(L L M, \mathcal{R}\left(q, \mathcal{D}=\left\{\mathcal{V}, \mathcal{E}_{\text {low }}, \mathcal{E}_{\text {high }}\right\}\right)\right),
$$

where hyperedges are used to represent relations. $\mathcal{E}_{\text {low }}$ denotes low-order pairwise entity relations, while $\mathcal{E}_{\text {high }}$ refers to high-order beyond pairwise multiple entities associations.

Theme-Aware Hypergraph Index The theme hypergraph is designed to model the semantic storyline structure of a document, establishing a narrative outline that provides cognitive guidance for subsequent detail retrieval.

Given a corpus $\mathcal{D}$, such as books, reports, or manuals, we first segment it into a set of chunks using a fixed-length sliding window with partial overlap to maintain semantic integrity, denoted as:

$$
\mathcal{D}=\left\{D_{1}, D_{2}, \ldots, D_{N}\right\},
$$

where $D_{i}$ denotes the $i$-th document chunk, serving as the basic unit for subsequent analysis.

Then, we perform semantic parsing on each chunk using LLMs to automatically extract its latent theme and associated key entities, thereby constructing a theme hypergraph. Specifically, we first employ predefined theme-level
extraction prompts $\mathcal{P}_{\text {ext_theme }}, \mathcal{P}_{\text {ext_key }}$ (detailed in Appendix) to guide the LLM in performing semantic parsing for each chunk $D_{i}$ and outputting the corresponding theme. Then, further extract the key entities related to the theme. The calculation process is as follows:

$$
\left\{\begin{array}{l}
\mathcal{E}_{\text {theme }}=\operatorname{LLM}\left(\mathcal{P}_{\text {ext_theme }}\left(D_{i}\right)\right) \\
\mathcal{V}_{\text {key }}=\operatorname{LLM}\left(\mathcal{P}_{\text {ext_key }}\left(D_{i}, \mathcal{E}_{\text {theme }}\right)\right)
\end{array} \quad \text { for } \quad D_{i} \in \mathcal{D}\right.
$$

Based on the extracted themes and entities, we can construct the theme hypergraph $\mathcal{G}_{\text {theme }}$, denoted as:

$$
\mathcal{G}_{\text {theme }}=\left\{\mathcal{V}_{\text {key }}, \mathcal{E}_{\text {theme }}\right\}
$$

where each hyperedge $\mathcal{E}_{\text {theme }}$ represents the narrative theme of the chunk, while the vertices $\mathcal{V}_{\text {key }}$ are the key entities.
Fine-Grained Entity Hypergraph Index After constructing the theme hypergraph, we obtain a global thematic structure among chunks. To further capture fine-grained multi-entity relations, we construct an entity hypergraph within each chunk to model high-order relations among entities, supporting subsequent fine-grained retrieval.

For each chunk $D_{i}$, we first extract entities (such as person, event, organization, etc.) and their descriptions using LLMs, which serve as the vertex set for the fine-grained entity hypergraph. Based on the semantic relations among these entities, we then construct two types of hyperedges: low-order hyperedges $\mathcal{E}_{\text {low }}$ capture basic pairwise relations, while high-order hyperedges $\mathcal{E}_{\text {high }}$ model more complex semantic associations among multiple entities, such as cooccurrence in events or causal links. The extraction process is represented as follows:

$$
\left\{\begin{array}{l}
\mathcal{V}=\operatorname{LLM}\left(\mathcal{P}_{\text {ext_entity }}\left(D_{i}\right)\right) \\
\mathcal{E}_{\text {low }}=\operatorname{LLM}\left(\mathcal{P}_{\text {ext_low }}\left(D_{i}, \mathcal{V}\right)\right) \quad \text { for } \quad D_{i} \in \mathcal{D} \\
\mathcal{E}_{\text {high }}=\operatorname{LLM}\left(\mathcal{P}_{\text {ext_high }}\left(D_{i}, \mathcal{V}\right)\right)
\end{array}\right.
$$

where $\mathcal{P}_{\text {ext_entity }}$ refers to the prompt designed for entity extraction from the text. $\mathcal{P}_{\text {ext_low }}$ and $\mathcal{P}_{\text {ext_high }}$ (detailed in Appendix) represent the extraction of paired and group relations from the obtained entities, respectively.

Finally, all extracted entities, along with their low-order and high-order relations, are organized into a fine-grained entity hypergraph $\mathcal{G}_{\text {entity }}$ and stored in a hypergraph database.

$$
\mathcal{G}_{\text {entity }}=\left\{\mathcal{V}, \mathcal{E}_{\text {low }}, \mathcal{E}_{\text {high }}\right\}
$$

## Cognitive-Inspired Two-Stage Retrieval

Motivated by the top-down information processing pattern observed in human memory retrieval, we design a cognitiveinspired two-stage retrieval strategy. Specifically, it first identify theme threads in the theme hypergraph related to the query. These threads then serve as cues to guide the retrieval of fine-grained information from the entity hypergraph.

For a given user query $q$, following (Guo et al. 2024; Feng et al. 2025a), we first extract two types of keywords: theme keywords (overarching concepts or themes) and entity keywords (specific entities or details), as follows:

$$
\mathcal{X}_{\text {theme }}, \mathcal{X}_{\text {entity }}=\operatorname{LLM}\left(\mathcal{P}_{\text {keyword }}(q)\right),
$$

where $\mathcal{X}_{*}=\left\{x_{1}, x_{2}, \ldots\right\}, \mathcal{P}_{\text {keyword }}$ is the prompt for extracting keywords from the query, detailed in Appendix.

Theme-Aware Hypergraph Retrieval Subsequently, based on the extracted keywords, we perform structured retrieval over the hypergraph database. It is worth noting that entity keywords primarily describe concrete individual information and are thus matched to vertices, while theme keywords reflect abstract semantic relations among multiple entities and are therefore used to retrieve relevant hyperedges. The natural combination of two types of keywords and hypergraph structure enhances both retrieval specificity and structural compatibility.

In the first stage of retrieval, the extracted theme keywords are used to perform semantic matching within the theme hypergraph, and selecting the top- k relevant theme hyperedges.

$$
\mathcal{E}_{\text {rel }}=\left\{\mathcal{R}\left(x_{i}, \mathcal{E}_{\text {theme }}\right) \mid x_{i} \in \mathcal{X}_{\text {theme }}\right\},
$$

where $\mathcal{E}_{\text {rel }}$ represents the relevant hyperedges retrieved from the vector database. Then, we perform a diffusion process over the hypergraph database to retrieve their neighboring vertices, providing additional context awareness of the retrieved theme.

$$
\mathcal{V}_{\mathrm{dif}}=\left\{\mathcal{N}\left(e_{i}, \mathcal{G}_{\mathrm{theme}}\right) \mid e_{i} \in \mathcal{E}_{\mathrm{rel}}\right\},
$$

where $\mathcal{N}$ denotes the function of obtaining the corresponding neighbors from the hypergraph. $\mathcal{V}_{\text {dif }}$ is the diffusion vertices. Then, both the $\mathcal{E}_{\text {rel }}$ and $\mathcal{V}_{\text {dif }}$, along with the corresponding textual contexts, are fed into LLMs as prior knowledge to generate an initial theme-aware answer as follows:

$$
\mathcal{A}_{\text {theme }}=\operatorname{LLM}\left(q, \mathcal{E}_{\text {rel }}, \mathcal{V}_{\text {dif }}, \mathcal{C}_{\text {e_rel }}, \mathcal{C}_{\text {v_dif }}\right)
$$

where $\mathcal{A}_{\text {theme }}$ denotes the output of query $q$ after retrieving from $\mathcal{G}_{\text {theme }}, \mathcal{C}_{*}$ is the corresponding context.

Theme-aligned Entity Hypergraph Retrieval After completing the initial theme-based retrieval, we further perform fine-grained information retrieval within the entity hypergraph. Guided by the retrieved themes, this section supplements entity-level semantic details, enabling effective alignment between local information and global themes.

Unlike the theme retrieval stage which targets hyperedges, this stage focuses on retrieving top-k vertices within the entity hypergraph by entity keywords, thereby achieving finegrained semantic supplement and structured alignment.

$$
\mathcal{V}_{\text {rel }}=\left\{\mathcal{R}\left(\mathcal{P}_{\text {align }}\left(x_{i}, \mathcal{A}_{\text {theme }}\right), \mathcal{V}_{\text {entity }}\right) \mid x_{i} \in \mathcal{X}_{\text {entity }}\right\}
$$

where $\mathcal{P}_{\text {align }}$ is the prompt (detailed in Appendix) used for embedding theme knowledge. $\mathcal{V}_{\text {rel }}$ refers the retrieved relevant entities. Then perform a hypergraph structure diffusion as follows:

$$
\mathcal{E}_{\text {dif }}=\left\{\mathcal{N}\left(v_{i}, \mathcal{G}_{\text {entity }}\right) \mid v_{i} \in \mathcal{V}_{\text {rel }}\right\}
$$

Finally, the retrieved $\mathcal{V}_{\text {rel }}$, diffusion $\mathcal{E}_{\text {dif }}$, and their corresponding contexts, integrated with the previous theme information $\mathcal{A}_{\text {theme }}$, to form a structured input for LLMs to generate the final answer $\mathcal{A}$ for query $q$, thereby achieving a comprehensive semantic generation process from theme guidance to detailed support.

$$
\mathcal{A}=\operatorname{LLM}\left(q, \mathcal{A}_{\text {theme }}, \mathcal{V}_{\text {rel }}, \mathcal{E}_{\text {dif }}, \mathcal{C}_{\text {v_rel }}, \mathcal{C}_{\text {e_dif }}\right)
$$

![](https://cdn.mathpix.com/cropped/7129fb25-f695-45a3-839a-459d6159495b-05.jpg?height=549&width=1694&top_left_y=184&top_left_x=210)
Figure 3: Test results by scoring. (a) is the comparison results on five datasets; (b) is the results of the neurology dataset on six dimensions; (c) shows the evaluation results on different LLMs.

## Experiments

## Experimental Setup

Datasets To systematically evaluate our method across diverse application scenarios, we adopt five datasets from two benchmarks: Mix, CS, and Agriculture from the UltraDomain benchmark (Qian et al. 2024), and Neurology and Pathology from the MIRAGE benchmark (Xiong et al. 2024). UltraDomain covers typical RAG applications across different domains, while MIRAGE focuses on medical question answering and domain-specific knowledge coverage. The statistical information is given in the Appendix.

Based on domain consistency and semantic correlation within the texts, we categorize the datasets into three types to enable a comprehensive analysis of the model's adaptability: Cross-domain Sparse (Mix): Fragmented passages from unrelated domains with weak semantic coherence. Intra-domain Sparse (CS, Agriculture): Domain-specific documents with weak inter-passage context. Intra-domain Dense (Neurology, Pathology): Highly structured medical textbooks with strong semantic continuity from MIRAGE. Additionally, we follow the data processing and query procedure of LightRAG, utilizing GPT-4o to generate complex, document-related queries.

Baselines We compared our approach with the state-of-the-art and popular RAG methods. Including text-base RAG: NaiveRAG (Gao et al. 2023), graph-enhanced RAG approaches: GraphRAG (Edge et al. 2024), LightRAG (Guo et al. 2024), HiRAG (Huang et al. 2025), hypergraphenhanced methods: Hyper-RAG (Feng et al. 2025a). The baseline details are provided in the Appendix.

Implementation Details To ensure fairness and consistency for both the baseline and proposed methods, we validate on five different LLMs for information extraction, question answering, including GPT-4o-mini (Achiam et al. 2023), Qwen-Plus (Yang et al. 2025), GLM-4-Air (GLM et al. 2024), DeepSeek-V3 (Liu et al. 2024), and LLaMa-3.3-70B (Dubey et al. 2024). The result evaluation is default on GPT-4o-mini, as well as the text-embedding-3-small em-
bedding model for vector encoding and retrieval tasks. Unless otherwise specified, all reported results are based on GPT-4o-mini.

Evaluation Metrics Following the recent works, we adopt two evaluation strategies: Selection-based (Guo et al. 2024; Huang et al. 2025) and Score-based (Wang et al. 2024; Feng et al. 2025a), providing both relative and absolute perspectives on model performance. The Selection-based evaluation uses LLMs to reports win rates of answer quality between two methods. The Score-based evaluation employs LLMs to score responses for different methods. Both strategies assess models from six dimensions: Comprehensiveness, Empowerment, Relevance, Consistency, Clarity, and Logical. We report both per-dimension and overall average scores. Detailed evaluation descriptions are in the Appendix.

## Main Results

Our primary results are presented in Table 1 and Figure 3, and more results are provided in the Appendix. Cog-RAG consistently outperforms all baselines across multiple dimensions. Additionally, we have several key insights:

1) Knowledge graphs can enhance RAG to model a broader scope of information. Graph-enhanced methods, represented by GraphRAG and LightRAG, demonstrate significant advantages over the conventional NaiveRAG, primarily due to the modeling of graph structures. In contrast, NaiveRAG relies solely on vector similarity and fails to account for these structured semantic relations. Hypergraphenhanced approaches, such as Hyper-RAG and Cog-RAG, offer a more comprehensive modeling of knowledge structures that extend beyond pairwise relations, demonstrating superior potential in knowledge representation.
2) Cog-RAG outperforms the baselines across all kinds of evaluation datasets and LLMs. For Selection-based results in Table 1, we can see that in cross-domain sparse settings, both Hyper-RAG and Cog-RAG utilize hypergraphs to capture high-order relations, resulting in an average improvement of over $10.0 \%$ compared to graph-based methods. In intra-domain sparse datasets, Cog-RAG outper-

| Mix |  |  | CS |  | Agriculture |  | Neurology |  | Pathology |  |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
|  | NaiveRAG | Cog-RAG | NaiveRAG | Cog-RAG | NaiveRAG | Cog-RAG | NaiveRAG | Cog-RAG | NaiveRAG | Cog-RAG |
| Comp. | 12.0\% | 88.0\% | 4.0\% | 96.0\% | 1.0\% | 99.0\% | 3.0\% | 97.0\% | 6.0\% | 94.0\% |
| Empo. | 10.0\% | 90.0\% | 3.0\% | 97.0\% | 2.0\% | 98.0\% | 1.0\% | 99.0\% | 4.0\% | 96.0\% |
| Rele. | 27.0\% | 73.0\% | 18.0\% | 82.0\% | 6.0\% | 94.0\% | 11.0\% | 89.0\% | 8.0\% | 92.0\% |
| Cons. | 10.0\% | 90.0\% | 4.0\% | 96.0\% | 1.0\% | 99.0\% | 2.0\% | 98.0\% | 4.0\% | 96.0\% |
| Clar. | 23.0\% | 77.0\% | 11.0\% | 89.0\% | 6.0\% | 94.0\% | 6.0\% | 94.0\% | 8.0\% | 92.0\% |
| Logi. | 11.0\% | 89.0\% | 5.0\% | 95.0\% | 1.0\% | 99.0\% | 1.0\% | 99.0\% | 5.0\% | 95.0\% |
| Overall | 15.5\% | 84.5\% | 7.5\% | 92.5\% | 2.8\% | 97.2\% | 3.2\% | 96.0\% | 5.8\% | 94.2\% |
|  | GraphRAG | Cog-RAG | GraphRAG | Cog-RAG | GraphRAG | Cog-RAG | GraphRAG | Cog-RAG | GraphRAG | Cog-RAG |
| Comp. | 40.0\% | 60.0\% | 36.0\% | 64.0\% | 32.0\% | 68.0\% | 34.0\% | 66.0\% | 32.0\% | 68.0\% |
| Empo. | 36.0\% | 64.0\% | 35.0\% | 65.0\% | 26.0\% | 74.0\% | 27.0\% | 73.0\% | 23.0\% | 77.0\% |
| Rele. | 45.0\% | 55.0\% | 39.0\% | 61.0\% | 35.0\% | 65.0\% | 37.0\% | 63.0\% | 31.0\% | 69.0\% |
| Cons. | 40.0\% | 60.0\% | 35.0\% | 65.0\% | 29.0\% | 71.0\% | 31.0\% | 69.0\% | 31.0\% | 69.0\% |
| Clar. | 46.0\% | 54.0\% | 36.0\% | 64.0\% | 38.0\% | 62.0\% | 36.0\% | 64.0\% | 30.0\% | 70.0\% |
| Logi. | 39.0\% | 61.0\% | 37.0\% | 63.0\% | 27.0\% | 73.0\% | 33.0\% | 67.0\% | 29.0\% | 71.0\% |
| Overall | 41.0\% | 59.0\% | 36.3\% | 63.7\% | 31.2\% | 68.8\% | 33.0\% | 67.0\% | 29.5\% | 70.5\% |
|  | LightRAG | Cog-RAG | LightRAG | Cog-RAG | LightRAG | Cog-RAG | LightRAG | Cog-RAG | LightRAG | Cog-RAG |
| Comp. | 38.0\% | 62.0\% | 30.0\% | 70.0\% | 23.0\% | 77.0\% | 28.0\% | 72.0\% | 30.0\% | 70.0\% |
| Empo. | 30.0\% | 70.0\% | 26.0\% | 74.0\% | 20.0\% | 80.0\% | 22.0\% | 78.0\% | 25.0\% | 75.0\% |
| Rele. | 36.0\% | 64.0\% | 27.0\% | 73.0\% | 25.0\% | 75.0\% | 28.0\% | 72.0\% | 32.0\% | 68.0\% |
| Cons. | 34.0\% | 66.0\% | 29.0\% | 71.0\% | 21.0\% | 79.0\% | 25.0\% | 75.0\% | 27.0\% | 73.0\% |
| Clar. | 38.0\% | 62.0\% | 24.0\% | 76.0\% | 22.0\% | 78.0\% | 26.0\% | 74.0\% | 26.0\% | 74.0\% |
| Logi. | 35.0\% | 65.0\% | 29.0\% | 71.0\% | 23.0\% | 77.0\% | 26.0\% | 74.0\% | 26.0\% | 74.0\% |
| Overall | 35.2\% | 64.8\% | 27.5\% | 72.5\% | 22.3\% | 77.7\% | 25.8\% | 74.2\% | 27.7\% | 72.3\% |
|  | HiRAG | Cog-RAG | HiRAG | Cog-RAG | HiRAG | Cog-RAG | HiRAG | Cog-RAG | HiRAG | Cog-RAG |
| Comp. | 44.0\% | 56.0\% | 40.0\% | 60.0\% | 41.0\% | 59.0\% | 35.0\% | 65.0\% | 40.0\% | 60.0\% |
| Empo. | 39.0\% | 61.0\% | 36.0\% | 64.0\% | 36.0\% | 64.0\% | 31.0\% | 69.0\% | 37.0\% | 63.0\% |
| Rele. | 45.0\% | 55.0\% | 47.0\% | 53.0\% | 44.0\% | 56.0\% | 35.0\% | 65.0\% | 41.0\% | 59.0\% |
| Cons. | 39.0\% | 61.0\% | 40.0\% | 60.0\% | 37.0\% | 63.0\% | 32.0\% | 68.0\% | 37.0\% | 63.0\% |
| Clar. | 45.0\% | 54.0\% | 50.0\% | 50.0\% | 44.0\% | 56.0\% | 31.0\% | 69.0\% | 40.0\% | 60.0\% |
| Logi. | 40.0\% | 60.0\% | 40.0\% | 60.0\% | 38.0\% | 62.0\% | 31.0\% | 69.0\% | 36.0\% | 64.0\% |
| Overall | 42.0\% | 58.0\% | 42.2\% | 57.8\% | 40.0\% | 60.0\% | 32.5\% | 67.5\% | 38.5\% | 61.5\% |
|  | Hyper-RAG | Cog-RAG | Hyper-RAG | Cog-RAG | Hyper-RAG | Cog-RAG | Hyper-RAG | Cog-RAG | Hyper-RAG | Cog-RAG |
| Comp. | 43.0\% | 57.0\% | 45.0\% | 55.0\% | 49.0\% | 51.0\% | 40.0\% | 60.0\% | 42.0\% | 58.0\% |
| Empo. | 42.0\% | 58.0\% | 43.0\% | 57.0\% | 40.0\% | 60.0\% | 37.0\% | 63.0\% | 37.0\% | 63.0\% |
| Rele. | 53.0\% | 47.0\% | 47.0\% | 53.0\% | 45.0\% | 55.0\% | 46.0\% | 54.0\% | 37.0\% | 63.0\% |
| Cons. | 43.0\% | 57.0\% | 44.0\% | 56.0\% | 44.0\% | 56.0\% | 38.0\% | 62.0\% | 36.0\% | 64.0\% |
| Clar. | 56.0\% | 44.0\% | 48.0\% | 52.0\% | 42.0\% | 58.0\% | 41.0\% | 59.0\% | 32.0\% | 68.0\% |
| Logi. | 44.0\% | 56.0\% | 46.0\% | 54.0\% | 43.0\% | 57.0\% | 35.0\% | 65.0\% | 37.0\% | 63.0\% |
| Overall | 46.8\% | 53.2\% | 45.5\% | 54.5\% | 43.8\% | 56.2\% | 39.5\% | 60.5\% | 36.8\% | 63.2\% |

Table 1: Average win rates of six evaluation metrics across five datasets. The comparison is made between baselines and CogRAG. Among them, we refer to the six metrics as Comp. (Comprehensiveness), Empo. (Empowerment), Rele. (Relevance), Cons. (Consistency), Clar. (Clarity), and Logi. (Logical).

forms HiRAG by $15.6 \%$ and $20.0 \%$, benefiting from multihyperedge propagation that uncovers latent themes and entity relations. In intra-domain dense medical corpora, CogRAG achieves the most significant gains. Through dual hypergraph modeling and cognitive-inspired retrieval, enhancing the alignment and aggregation of theme and fine-grained details. Compared to Hyper-RAG, it improves by $21.0 \%$ and $26.4 \%$, respectively. For Score-based results, Figure 3 objectively presents the evaluation results across six dimensions on five LLMs. The results demonstrate that Cog-RAG achieves consistent and significant improvements over base-
line methods in all dimensions. Moreover, when applying different LLMs for indexing and answering, it still exhibits clear advantages, highlighting its structural effectiveness.
3) Dual-hypergraph alignment enhances knowledge representation and semantic consistency. Inspired by human top-down cognitive pathways, Cog-RAG utilizes a dual hypergraph structure to align macro to micro knowledge. Specifically, for Selection-based results in Intra-domain Dense scenario, Cog-RAG improves by $35.0 \%$ and $23.0 \%$ compared to the entity-level hierarchical method HiRAG. For Score-based results, Cog-RAG outperforms HiRAG and

| Models | Mix | CS | Neurology |
| :--- | :--- | :--- | :--- |
|  | (Overall) | (Overall) | (Overall) |
| Cog-RAG | 85.39 | 87.07 | 86.55 |
| w./o. Entity Hypergraph | 76.58 | 84.58 | 84.49 |
| w./o. Theme Hypergraph | 84.82 | 85.88 | 85.41 |
| w./o. Two-Stage Retrieval | 84.88 | 86.41 | 86.18 |

Table 2: Ablation study on different datasets by scoring, where w./o. indicates without the part of the method.

Hyper-RAG by 1.37 and 1.20 on neurology datasets, significantly enhancing the model's ability to handle knowledgeintensive domains and ensuring semantic consistency.

## Ablation Study

This section conducts ablation studies to evaluate the contribution of each core component in Cog-RAG: the theme, entity hypergraph, and two-stage retrieval strategy. Table 2 shows the results by Scoring-based evaluation, and Selection-based results can be found in the Appendix. The results from three types of representative datasets are summarized below.

1) Effectiveness of the Entity Hypergraph. Removing the entity hypergraph leads to a significant decrease in performance on all three types of datasets, especially on the Mix dataset. This indicates its critical role in capturing finegrained semantic relations within chunks. This effect is consistently observed across domains, confirming that intrachunk entity-level modeling can enhance the representation of local knowledge.
2) Effectiveness of the Theme Hypergraph. Excluding the theme hypergraph causes a moderate decrease (drop 1.19 on CS and 1.14 on Neurology), highlighting its role in modeling global theme structures across chunks. The benefit is particularly noticeable in intra-domain tasks, where maintaining coherent theme alignment helps with cross-chunk reasoning and retrieval. However, on the Mix dataset, using only the theme hypergraph leads to performance degradation (from 85.39 to 76.58 ), indicating that in cross-domain sparse and weakly structured scenarios, theme relations may introduce noise that interferes with retrieval and answering.
3) Effectiveness of the Two-Stage Retrieval. Bypassing this component (by directly concatenating information from both the theme and entity hypergraphs and inputting it into LLMs) leads to consistent performance drops. This highlights the importance of the two-stage retrieval, especially in knowledge-intensive scenarios where global semantic guidance followed by entity-level refinement enables more accurate and coherent retrieval.

## Hypergraph Visualization

In the Neurology dataset, Figure 4 visualized the relations of Sleep Apnea in the entity hypergraph. It illustrates the complex relations between Sleep Apnea and multiple related entities such as Chronic Lung Disease, Headache, and Respiratory Centers. It captures not only pairwise relations but

![](https://cdn.mathpix.com/cropped/7129fb25-f695-45a3-839a-459d6159495b-07.jpg?height=461&width=839&top_left_y=184&top_left_x=1102)
Figure 4: Entity Hypergraph Visualization.

also reveals multi-entity dependencies beyond pairs. As observed, the complex hypergraph among Hypertension, Sleep Apnea, Kyphoscoliosis, and Muscular Dystrophy illustrates various health risks and respiratory challenges connected to sleep quality and disorders, affecting overall wellness.

## Why is Cog-RAG Effective?

## Theme-Aligned vs. Graph / Hypergraph Index

Graph and hypergraph-enhanced RAG mainly focus on modeling local entity-level relations within document chunks, making them less effective for tasks that require global semantic reasoning. In contrast, Cog-RAG introduces a dual-hypergraph structure that supports alignment from global themes to fine-grained entities, leading to improved contextual grounding and response consistency. Notably, our analysis reveals that the theme hypergraph is particularly beneficial in structured, domain-specific settings, while it may introduce noise in loosely structured, open-domain scenarios. This suggests further opportunities for dynamic filtering and graph construction.

## Cognitive-Inspired vs. Conventional Retrieval

Conventional RAG systems rely on single-stage retrieval, which merges all retrieved content into LLMs. This design often leads to incomplete or noisy evidence aggregation for complex knowledge-intensive tasks. The cognitiveinspired two-stage retrieval strategy enables top-down semantic alignment and aggregation, providing more accurate knowledge support and reducing redundant information.

## Conclusion

Inspired by human cognitive pathways, this paper introduces Cog-RAG, which enhances LLM responses by integrating dual-hypergraph structures and a cognitive-inspired two-stage retrieval mechanism. Cog-RAG enables hierarchical knowledge modeling and semantic alignment at both macro-thematic and micro-entity levels, addressing issues of information loss and semantic gaps inherent in graph-based methods. Experimental results show that Cog-RAG significantly outperforms state-of-the-art methods across various types of datasets on knowledge-intensive tasks.

## Acknowledgement

This work was supported by the National Natural Science Foundation of China under Grant Nos. 62088102 and U24A20252, the Key Research and Development Program of Shaanxi Province of China under Grant Nos. 2024PT-ZCK-66 and 2024CY2-GJHX-48.

## References

Achiam, J.; Adler, S.; Agarwal, S.; Ahmad, L.; Akkaya, I.; Aleman, F. L.; Almeida, D.; Altenschmidt, J.; Altman, S.; Anadkat, S.; et al. 2023. Gpt-4 technical report. arXiv:2303.08774.
Asai, A.; Wu, Z.; Wang, Y.; Sil, A.; and Hajishirzi, H. 2023. Self-rag: Learning to retrieve, generate, and critique through self-reflection. In The Twelfth International Conference on Learning Representations.
Ayala, O.; and Bechard, P. 2024. Reducing hallucination in structured outputs via Retrieval-Augmented Generation. Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track), 228-238.
Chen, B.; Guo, Z.; Yang, Z.; Chen, Y.; Chen, J.; Liu, Z.; Shi, C.; and Yang, C. 2025. Pathrag: Pruning graphbased retrieval augmented generation with relational paths. arXiv:2502.14902.
Cheng, Y.; Zhao, Y.; Zhu, J.; Liu, Y.; Sun, X.; and Li, X. 2025. Human Cognition Inspired RAG with Knowledge Graph for Complex Problem Solving. arXiv:2503.06567.
Dong, G.; Song, X.; Zhu, Y.; Qiao, R.; Dou, Z.; and Wen, J.-R. 2025. Toward verifiable instruction-following alignment for retrieval augmented generation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, 23796-23804. Philadelphia, Pennsylvania, USA.
Dubey, A.; Jauhri, A.; Pandey, A.; Kadian, A.; Al-Dahle, A.; Letman, A.; Mathur, A.; Schelten, A.; Yang, A.; Fan, A.; et al. 2024. The llama 3 herd of models. arXiv:2407.21783.
Edge, D.; Trinh, H.; Cheng, N.; Bradley, J.; Chao, A.; Mody, A.; Truitt, S.; Metropolitansky, D.; Ness, R. O.; and Larson, J. 2024. From local to global: A graph rag approach to query-focused summarization. arXiv:2404.16130.
Fan, W.; Ding, Y.; Ning, L.; Wang, S.; Li, H.; Yin, D.; Chua, T.-S.; and Li, Q. 2024. A survey on rag meeting llms: Towards retrieval-augmented large language models. In Proceedings of the 30th ACM SIGKDD conference on knowledge discovery and data mining, 6491-6501. New York, NY, USA.
Feng, Y.; Hu, H.; Hou, X.; Liu, S.; Ying, S.; Du, S.; Hu, H.; and Gao, Y. 2025a. Hyper-RAG: Combating LLM Hallucinations using Hypergraph-Driven Retrieval-Augmented Generation. arXiv:2504.08758.
Feng, Y.; Yang, C.; Hou, X.; Du, S.; Ying, S.; Wu, Z.; and Gao, Y. 2025b. Beyond Graphs: Can Large Language Models Comprehend Hypergraphs? In The Thirteenth International Conference on Learning Representations, 4246842495. Singapore.

Gao, Y.; Feng, Y.; Ji, S.; and Ji, R. 2022. HGNN ${ }^{+}$: General hypergraph neural networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(3): 3181-3199.
Gao, Y.; Xiong, Y.; Gao, X.; Jia, K.; Pan, J.; Bi, Y.; Dai, Y.; Sun, J.; Wang, H.; and Wang, H. 2023. Retrievalaugmented generation for large language models: A survey. arXiv:2312.10997.
GLM, T.; Zeng, A.; Xu, B.; Wang, B.; Zhang, C.; Yin, D.; Zhang, D.; Rojas, D.; Feng, G.; Zhao, H.; et al. 2024. Chatglm: A family of large language models from glm-130b to glm-4 all tools. arXiv:2406.12793.
Guo, Z.; Xia, L.; Yu, Y.; Ao, T.; and Huang, C. 2024. Lightrag: Simple and fast retrieval-augmented generation. arXiv:2410.05779.
Gutiérrez, B. J.; Shu, Y.; Gu, Y.; Yasunaga, M.; and Su, Y. 2024. HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models. In Advances in Neural Information Processing Systems, 59532-59569. Red Hook, NY, USA.
Han, X.; Xue, R.; Feng, J.; Feng, Y.; Du, S.; Shi, J.; and Gao, Y. 2025. Hypergraph foundation model for brain disease diagnosis. IEEE Transactions on Neural Networks and Learning Systems, 1-15.
Huang, H.; Huang, Y.; Yang, J.; Pan, Z.; Chen, Y.; Ma, K.; Chen, H.; and Cheng, J. 2025. Retrieval-Augmented Generation with Hierarchical Knowledge. arXiv:2503.10150.
Ji, S.; Feng, Y.; Ji, R.; Zhao, X.; Tang, W.; and Gao, Y. 2020. Dual channel hypergraph collaborative filtering. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery \& data mining, 2020-2029. New York, NY, USA.
Lewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V.; Goyal, N.; Küttler, H.; Lewis, M.; Yih, W.-t.; Rocktäschel, T.; et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33: 9459-9474.
Li, R.; and Du, X. 2023. Leveraging structured information for explainable multi-hop question answering and reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2023, 6779-6789. Singapore.
Li, Z.; Chen, X.; Yu, H.; Lin, H.; Lu, Y.; Tang, Q.; Huang, F.; Han, X.; Sun, L.; and Li, Y. 2024. Structrag: Boosting knowledge intensive reasoning of 11 ms via inference-time hybrid information structurization. arXiv:2410.08815.
Liu, A.; Feng, B.; Xue, B.; Wang, B.; Wu, B.; Lu, C.; Zhao, C.; Deng, C.; Zhang, C.; Ruan, C.; et al. 2024. Deepseek-v3 technical report. arXiv:2412.19437.
Luo, H.; Chen, G.; Zheng, Y.; Wu, X.; Guo, Y.; Lin, Q.; Feng, Y.; Kuang, Z.; Song, M.; Zhu, Y.; et al. 2025. HyperGraphRAG: Retrieval-Augmented Generation via Hypergraph-Structured Knowledge Representation. arXiv:2503.21322.
Peng, B.; Zhu, Y.; Liu, Y.; Bo, X.; Shi, H.; Hong, C.; Zhang, Y.; and Tang, S. 2024. Graph retrieval-augmented generation: A survey. arXiv:2408.08921.

Qian, H.; Zhang, P.; Liu, Z.; Mao, K.; and Dou, Z. 2024. Memorag: Moving towards next-gen rag via memoryinspired knowledge discovery. arXiv:2409.05591.
Sarmah, B.; Mehta, D.; Hall, B.; Rao, R.; Patel, S.; and Pasquali, S. 2024. Hybridrag: Integrating knowledge graphs and vector retrieval augmented generation for efficient information extraction. In Proceedings of the 5th ACM International Conference on AI in Finance, 608-616. New York, NY, USA.
Sun, X.; Cheng, H.; Liu, B.; Li, J.; Chen, H.; Xu, G.; and Yin, H. 2023. Self-supervised hypergraph representation learning for sociological analysis. IEEE Transactions on Knowledge and Data Engineering, 35(11): 11860-11871.
Wang, M.; Chen, L.; Fu, C.; Liao, S.; Zhang, X.; Wu, B.; Yu, H.; Xu, N.; Zhang, L.; Luo, R.; et al. 2024. Leave no document behind: Benchmarking long-context 11 ms with extended multi-doc qa. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, 5627-5646. Miami, Florida, USA.
Wang, S.; Fang, Y.; Zhou, Y.; Liu, X.; and Ma, Y. 2025. ArchRAG: Attributed Community-based Hierarchical Retrieval-Augmented Generation. arXiv:2502.09891.
Xia, Y.; Zhou, J.; Shi, Z.; Chen, J.; and Huang, H. 2025. Improving retrieval augmented language model with selfreasoning. In Proceedings of the AAAI conference on artificial intelligence, volume 39, 25534-25542.
Xiong, G.; Jin, Q.; Lu, Z.; and Zhang, A. 2024. Benchmarking retrieval-augmented generation for medicine. In Findings of the Association for Computational Linguistics ACL 2024, 6233-6251. Bangkok, Thailand.
Yang, A.; Li, A.; Yang, B.; Zhang, B.; Hui, B.; Zheng, B.; Yu, B.; Gao, C.; Huang, C.; Lv, C.; et al. 2025. Qwen3 technical report. arXiv:2505.09388.
Yang, X.; Sun, K.; Xin, H.; Sun, Y.; Bhalla, N.; Chen, X.; Choudhary, S.; Gui, R.; Jiang, Z.; Jiang, Z.; et al. 2024. Crag-comprehensive rag benchmark. In Advances in Neural Information Processing Systems, volume 37, 10470-10490. Red Hook, NY, USA.
Zhang, L.; Yu, Y.; Wang, K.; and Zhang, C. 2024. ARL2: Aligning Retrievers with Black-box Large Language Models via Self-guided Adaptive Relevance Labeling. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics, 3708-3719. Bangkok, Thailand.
Zhang, Q.; Chen, S.; Bei, Y.; Yuan, Z.; Zhou, H.; Hong, Z.; Dong, J.; Chen, H.; Chang, Y.; and Huang, X. 2025. A survey of graph retrieval-augmented generation for customized large language models. arXiv:2501.13958.

| APPENDIX |  |  |  |  |  |
| :--- | :--- | :--- | :--- | :--- | :--- |
| A. Experimental Datasets |  |  |  |  |  |
| Statistics | Mix | CS | Agriculture | Neurology | Pathology |
| Total Documents | 61 | 10 | 12 | 1 | 1 |
| Total Chunks | 560 | 1992 | 1813 | 1790 | 824 |
| Total Tokens | 615,355 | 2,190,803 | 1,993,515 | 1,968,716 | 905,760 |

Table 3: Statistical information of the datasets

Table 1 presents the statistical information of the five datasets.

## B. Details of Baselines

We compared our approach with the state-of-the-art and popular RAG methods. The specific description of the method is as follows:

NaiveRAG: The baseline of the standard RAG systems, which segments texts into chunks and stores them as embeddings in a vector database. During retrieval, relevant chunks are directly matched via vector similarity.

GraphRAG: A standard graph-enhanced RAG method that employs LLMs to extract entities and relations from texts as nodes and edges of the graph. Entities are further clustered to generate community reports, which are traversed during retrieval to obtain global information.

LightRAG: A graph-enhanced RAG method that integrates graph structures with vector-based representations. It employs a dual-level retrieval strategy to retrieve information from both nodes and edges within the graph knowledge.

HiRAG: A graph-enhanced RAG approach that builds a hierarchical graph via multi-level semantic clustering, enabling hierarchical indexing and retrieval of text knowledge.

Hyper-RAG: A standard Hypergraph-enhanced RAG method that uses hyperedges to represent both paired loworder relations and beyond-paired high-order relations.

## C. Details of Evaluation Metrics

We evaluated the proposed method and baseline from six dimensions, including Comprehensiveness, Empowerment, Relevance, Consistency, Clarity, and Logical coherence.

For Selection-based evaluation, we report win rates to conduct a qualitative comparison. We also alternated the answer order of each pair of methods in the prompts and calculated the average result for a fair comparison. The metrics details are described below:

Comprehensiveness: How much detail does the answer provide to cover all aspects and details of the question? Empowerment: How well does the answer help the reader understand and make informed judgments about the topic? Relevance: How precisely does the answer address the core aspects of the question without including unnecessary information? Consistency: How well does the system integrate and synthesize information from multiple sources into a logically flowing response? Clarity: How well does the system
provide complete information while avoiding unnecessary verbosity and redundancy? Logical: How well does the system maintain consistent logical arguments without contradicting itself across the response?

For Score-based evaluation, we use LLMs to score responses for quantitative assessment, with the following specific indicators:

Comprehensiveness (0-100): Measure whether the answer comprehensively covers all key aspects of the question and whether there are omissions. Empowerment (0100): Measure the credibility of the answer and whether it convinces the reader that it is correct. High confidence answers often cite authoritative sources or provide sufficient evidence. Relevance (0-100): Measure whether the content of the answer is closely related to the question, and whether it stays focused on the topic without digression. Consistency (0-100): Measure whether the answer is logically organized, flows smoothly, and whether the parts of the answer are well connected and mutually supportive. Clarity (0-100): Measure whether the answer is expressed in a clear, unambiguous, and easily understandable manner, using appropriate language and definitions. Logical (0-100): Measure whether the answer is coherent, clear, and easy to understand.

For each evaluation dimension, we define five discrete rating levels, each associated with clear scoring criteria to ensure consistency and transparency. As an illustration, the Comprehensiveness dimension is rated as follows: Level 1 (0-20): The answer is extremely one-sided, leaving out key parts or important aspects of the question. Level 2 (20-40): The answer has some content but misses many important aspects and is not comprehensive enough. Level 3 (40-60): The answer is more comprehensive, covering the main aspects of the question, but there are still some omissions. Level 4 (60-80): The answer is comprehensive, covering most aspects of the question with few omissions. Level 5 (80-100): The answer is extremely comprehensive, covering all aspects of the question with no omissions, enabling the reader to gain a complete understanding.

## D. Additional Experiment Results

## D. 1 Comparison Experiment Results

Figure 1 shows the results of comparison experiments under different evaluation dimensions on the Mix, CS, Agriculture, and Pathology datasets.

## D. 2 Ablation Experiment Results

Table 2 presents the results of the selection-based evaluation on three types of representative datasets, including Mix, CS, and Neurology. The results also show the effectiveness of each component in Cog-RAG.

## E. Retrieval Efficiency Analysis

We conduct a comparative analysis of retrieval efficiency. Figure 2 illustrates the trade-off between retrieval time and final answer score across different methods. Notably, CogRAG achieves the highest overall score while maintaining low retrieval overhead, highlighting its superior balance between performance and efficiency.

![](https://cdn.mathpix.com/cropped/7129fb25-f695-45a3-839a-459d6159495b-11.jpg?height=946&width=1019&top_left_y=182&top_left_x=552)
Figure 5: Comparison results on different metrics.

|  | CS |  | Mix |  | Neurology |  |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
|  | w./o. Entity Hypergraph | Cog-RAG | w./o. Entity Hypergraph | Cog-RAG | w./o. Entity Hypergraph | Cog-RAG |
| Comp. | 24.0\% | 76.0\% | 34.0\% | 66.0\% | 30.0\% | 70.0\% |
| Empo. | 20.0\% | 80.0\% | 41.0\% | 59.0\% | 23.0\% | 77.0\% |
| Rele. | 29.0\% | 71.0\% | 31.0\% | 69.0\% | 30.0\% | 70.0\% |
| Cons. | 24.0\% | 76.0\% | 41.0\% | 59.0\% | 26.0\% | 74.0\% |
| Clar. | 25.0\% | 75.0\% | 25.0\% | 75.0\% | 30.0\% | 70.0\% |
| Logi. | 24.0\% | 76.0\% | 44.0\% | 56.0\% | 28.0\% | 72.0\% |
| Overall | 24.0\% | 76.0\% | 36.0\% | 64.0\% | 28.0\% | 72.0\% |
|  | w./o. Theme Hypergraph | Cog-RAG | w./o. Theme Hypergraph | Cog-RAG | w./o. Theme Hypergraph | Cog-RAG |
| Comp. | 46.0\% | 54.0\% | 41.0\% | 59.0\% | 43.0\% | 57.0\% |
| Empo. | 45.0\% | 55.0\% | 39.0\% | 61.0\% | 39.0\% | 61.0\% |
| Rele. | 47.0\% | 53.0\% | 39.0\% | 61.0\% | 42.0\% | 58.0\% |
| Cons. | 45.0\% | 55.0\% | 41.0\% | 59.0\% | 41.0\% | 59.0\% |
| Clar. | 57.0\% | 43.0\% | 39.0\% | 61.0\% | 41.0\% | 59.0\% |
| Logi. | 48.0\% | 52.0\% | 40.0\% | 60.0\% | 41.0\% | 59.0\% |
| Overall | 48.0\% | 52.0\% | 40.0\% | 60.0\% | 41.0\% | 59.0\% |
|  | w./o. Two-Stage Retrieval | Cog-RAG | w./o. Two-Stage Retrieval | Cog-RAG | w./o. Two-Stage Retrieval | Cog-RAG |
| Comp. | 44.0\% | 56.0\% | 46.0\% | 54.0\% | 43.0\% | 57.0\% |
| Empo | 40.0\% | 60.0\% | 44.0\% | 56.0\% | 40.0\% | 60.0\% |
| Rele. | 51.0\% | 49.0\% | 45.0\% | 55.0\% | 39.0\% | 61.0\% |
| Cons. | 39.0\% | 61.0\% | 47.0\% | 53.0\% | 44.0\% | 56.0\% |
| Clar. | 50.0\% | 50.0\% | 49.0\% | 51.0\% | 49.0\% | 51.0\% |
| Logi. | 38.0\% | 62.0\% | 47.0\% | 53.0\% | 45.0\% | 55.0\% |
| Overall | 44.0\% | 56.0\% | 46.0\% | 54.0\% | 43.0\% | 57.0\% |

Table 4: Average win rates on three datasets. The comparison is made between ablation and Cog-RAG.

![](https://cdn.mathpix.com/cropped/7129fb25-f695-45a3-839a-459d6159495b-12.jpg?height=549&width=708&top_left_y=182&top_left_x=249)
Figure 6: Comparison results on different metrics.

## F. Prompt Templates in Cog-RAG

## F. 1 Extracting Themes, Key Entities

## Extracting Themes

Formulation: $\mathcal{P}_{\text {ext_theme }}$
Prompt: Summarize the primary theme of the text document. This summary should capture the essence of the document's core conflict, main idea, or narrative arc. Ensure that the summary highlights key moments, changes, or shifts in the document, extract the following information:

- theme_description: A sentence that describes the primary theme of the text document, reflecting the main conflict, resolution, or key message.


## Extracting Key Entities

Formulation: $\mathcal{P}_{\text {ext_theme }}$
Prompt: From the theme identified in $\mathcal{P}_{\text {ext_key }}$, identify all key entities in each text document.
For each identified key entity, extract the following information:

- key_entity_name: Name of the key entity, use same language as input text. If English, capitalized the name.
- key_entity_type: Type of the key entity, such as person, concept, object, event, emotion, symbol.
- key_entity_description: Comprehensive description of the entity's attributes and activities.
- key_score: A score from 0 to 100 indicating the importance of the entity in the text.


## F. 2 Extracting Entities, Relations and Keywords

## Extracting Entities

Formulation: $\mathcal{P}_{\text {ext_entity }}$
Prompt: Identify all entities. For each identified entity, extract the following information:

- entity_name: Name of the entity, use same language as input text. If English, capitalized the name.
- entity_type: One of the following types: organization, person, geo, event, role, concept.
- entity_description: Comprehensive description of the entity's attributes and activities.
- additional_properties: Other attributes possibly associated with the entity, like time, space, emotion, motivation, etc.


## Extracting Low-order Relations

Formulation: $\mathcal{P}_{\text {ext_low }}$
Prompt: From the entities identified in $\mathcal{P}_{\text {ext_entity, }}$ identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.
For each pair of related entities, extract the following information:

- entities_pair: The name of source entity and target entity, as identified in $\mathcal{P}_{\text {ext_entity. }}$
- low_order_relationship_description: Explanation as to why you think the source entity and the target entity are related to each other.
- low_order_relationship_keywords: Keywords that summarize the overarching nature of the relationship, focusing on concepts or themes rather than specific details.
- low_order_relationship_strength: A numerical score indicating the strength of the relationship between the entities.


## Extracting Keywords from Query

Formulation: $\mathcal{P}_{\text {keywords }}$
Prompt: You are a helpful assistant tasked with identifying both theme-level and entity-level keywords in the user's query.
-Goal-
Given the query and its related theme, please refer to the theme, list both theme-level and entity-level keywords. theme-level keywords focus on overarching concepts or themes, while entity-level keywords focus on specific entities, details, or concrete terms.

## Extracting High-order Relations

Formulation: $\mathcal{P}_{\text {ext_high }}$
Prompt: For the entities identified in $\mathcal{P}_{\text {ext_entity }}$, based on the entity pair relationships in $\mathcal{P}_{\text {ext_low }}$, find connections or commonalities among multiple entities and construct high-order associated entity set as much as possible.
Extract the following information from all related entities, entity pairs:

- entities_set: The collection of names for elements in high-order associated entity set. high_order_relationship_description: Use the relationships among the entities in the set to create a detailed, smooth, and comprehensive description that covers all entities in the set, without leaving out any relevant information.
- high_order_relationship_generalization: Summarize the content of the entity set as concisely as possible.
- high_order_relationship_keywords: Keywords that summarize the overarching nature of the high-order association, focusing on concepts or themes rather than specific details.
- high_order_relationship_strength: A numerical score indicating the strength of the association among the entities in the set.


## F. 3 Theme Alignment to Entity

## Theme Alignment to Entity

Formulation: $\mathcal{P}_{\text {algin }}$
Prompt: Through the existing analysis, we can know the potential theme answer and entity-level keywords are $\mathcal{A}_{\text {theme }}$ and $\mathcal{X}_{\text {ext_entity. }}$ Please refer to the theme answer and entity-level keywords, combined with your own analysis, to select useful and relevant information to help you answer accurately.

## F. 4 Evaluation Metrics

## Selection-based Evaluation

Formulation: $\mathcal{P}_{\text {eval_scoring }}\left(q, \mathcal{A}_{1}, \mathcal{A}_{2}\right)$
$q$ denotes user query, $\mathcal{A}_{1}$ and $\mathcal{A}_{2}$ denotes the response from two approaches.
Prompt: You will evaluate two answers to the same question based on six criteria: Comprehensiveness, Empowerment, Relevance, Consistency, Clarity, and Logical.

## -Goal-

You will evaluate two answers to the same question by using the relevant documents based on six criteria: Comprehensiveness, Empowerment, Relevance, Consistency, Clarity, and Logical.
-Comprehensiveness: How much detail does the answer provide to cover all aspects and details of the question?
-Empowerment: How well does the answer help the reader understand and make informed judgments about the topic?
...,
-Logical: How well does the system maintain consistent logical arguments without contradicting itself across the response?

For each criterion, choose the better answer (either Answer 1 or Answer 2) and explain why. Then, select an overall winner based on these six categories.

Here are the question: $q$
Here are the two answers:
Answer 1: $\mathcal{A}_{1}$;
Answer 2: $\mathcal{A}_{2}$
Evaluate both answers using the six criteria listed above and provide detailed explanations for each criterion.

## Scoring-based Evaluation

Formulation: $\mathcal{P}_{\text {eval_scoring }}\left(q, \mathcal{A}, \mathcal{T}_{o}\right)$
$q$ denotes user query, $\mathcal{A}$ denotes LLM response, $\mathcal{T}_{o}$ denotes the original text chunk that generated the question.
Prompt: You are an expert tasked with evaluating answers to the questions by using the relevant documents based on five criteria: Comprehensiveness, Diversity, Empowerment, Logical, and Readability.

## -Goal-

You will evaluate tht answers to the questions by using the relevant documents based on on six criteria: Comprehensiveness, Empowerment, Relevance, Consistency, Clarity, and Logical.
-Comprehensiveness-
Measure whether the answer comprehensively covers all key aspects of the question and whether there are omissions.
Level | score range | description
Level $1|0-20|$ The answer is extremely one-sided, leaving out key parts or important aspects of the question.
Level $2|20-40|$ The answer has some content, but it misses many important aspects of the question and is not comprehensive enough.
Level $3|40-60|$ The answer is more comprehensive, covering the main aspects of the question, but there are still some omissions.
Level $4|60-80|$ The answer is comprehensive, covering most aspects of the question, with few omissions.
Level $5|80-100|$ The answer is extremely comprehensive, covering all aspects of the question with no omissions, enabling the reader to gain a complete understanding.
...,
For each indicator, please give the problem a corresponding Level based on the description of the indicator, and then give a score according to the score range of the level.

Here are the question: $q$
Here are the relevant document: $\mathcal{T}_{o}$
Here are the answer: $\mathcal{A}$
Evaluate all the answers using the six criteria listed above, for each criterion, provide a summary description, give a Level based on the description of the indicator, and then give a score based on the score range of the level.


[^0]:    *Corresponding Authors.
    Copyright © 2026, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.

